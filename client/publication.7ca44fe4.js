import{S as e,i as n,s as l,a,e as o,q as r,d as i,c as t,b as s,f as h,j as g,k as p,l as u,n as f}from"./client.f8a4939d.js";var c='<h2 id="preprints">Preprints</h2>\n<ul>\n<li><p><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2006.10968">The Normal-Generalised Gamma-Pareto process: A novel pure-jump Lévy process with flexible tail and jump-activity properties</a></p>\n<ul>\n<li>Fadhel Ayed, <strong>Juho Lee</strong>, François Caron</li>\n<li>arXiv:2006.10968</li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1909.13433">Deep amortized clustering</a></p>\n<ul>\n<li><strong>Juho Lee</strong>, Yoonho Lee, Yee Whye Teh</li>\n<li>arXiv:1909.13433</li>\n<li>A preliminary version of this work has been accepted to NeurIPS 2019 Sets &amp; Partitions workshop as an oral presentation.</li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1905.10733">A unified construction for series representations and finite approximations of completely random measures</a></p>\n<ul>\n<li><strong>Juho Lee</strong>, Xenia Miscouridou, François Caron</li>\n<li>arXiv:1905.10733</li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1805.10896v3">Adaptive network sparsification with dependent variational beta-Bernoulli dropout</a></p>\n<ul>\n<li><strong>Juho Lee</strong>, Saehoon Kim, Jaehong Yoon, Hae Beom Lee, Eunho Yang, Sung Ju Hwang</li>\n<li>arXiv:1805.10896</li>\n</ul>\n</li>\n</ul>\n<h2 id="conferences">Conferences</h2>\n<ul>\n<li><p><a target="_blank" rel="nofollow" href="https://openreview.net/forum?id=YVPBh4k78iZ">Scale Mixtures of Neural Network Gaussian Processes</a></p>\n<ul>\n<li><strong>Hyungi Lee</strong>, <strong>Eunggu Yun</strong>, Hongseok Yang, <strong>Juho Lee</strong></li>\n<li>ICLR 2022</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2107.01408">arXiv</a>, <a target="_blank" rel="nofollow" href="https://github.com/Hyungi-Lee/Scale-Mixtures-of-Neural-Network-Gaussian-Processes">code</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://openreview.net/forum?id=ivQruZvXxtz">Sequential Reptile: inter-task gradient alignment for multilingual learning</a></p>\n<ul>\n<li>Seanie Lee, Hae Beom Lee, <strong>Juho Lee</strong>, Sung Ju Hwang</li>\n<li>ICLR 2022</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2110.02600">arXiv</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://openreview.net/forum?id=GQd7mXSPua">Meta learning low rank covariance factors for energy-based deterministic uncertainty</a></p>\n<ul>\n<li>Jeffrey Ryan Willette, Hae Beom Lee, <strong>Juho Lee</strong>, Sung Ju Hwang</li>\n<li>ICLR 2022</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2110.06381">arXiv</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://papers.nips.cc/paper/2021/hash/466473650870501e3600d9a1b4ee5d44-Abstract.html">Diversity matters when learning from ensembles</a></p>\n<ul>\n<li><strong>Giung Nam</strong>*, <strong>Jongmin Yoon</strong>*, Yoonho Lee, <strong>Juho Lee</strong> (*: Equal Contribution)</li>\n<li>NeurIPS 2021</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2110.14149">arXiv</a>, <a target="_blank" rel="nofollow" href="https://github.com/cs-giung/giung2/tree/main/projects/Diversity-Matters">code</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://papers.nips.cc/paper/2021/hash/b24d516bb65a5a58079f0f3526c87c57-Abstract.html">Mini-batch consistent slot set encoder for scalable set encoding</a></p>\n<ul>\n<li>Andreis Bruno, Jeffrey Ryan Willette, <strong>Juho Lee</strong>, Sung Ju Hwang</li>\n<li>NeurIPS 2021</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2103.01615">arXiv</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://openaccess.thecvf.com/content/ICCV2021/html/Liu_A_Multi-Mode_Modulator_for_Multi-Domain_Few-Shot_Classification_ICCV_2021_paper.html">A multi-mode modulator for multi-domain few-shot classification</a></p>\n<ul>\n<li>Yanbin Liu, <strong>Juho Lee</strong>, Linchao Zhu, Ling Chen, Humphrey Shi, Yi Yang</li>\n<li>ICCV 2021</li>\n<li><a target="_blank" rel="nofollow" href="https://github.com/csyanbin/tri-M-ICCV">code</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://proceedings.mlr.press/v139/yoon21a.html">Adversarial purification with score-based generative models</a></p>\n<ul>\n<li><strong>Jongmin Yoon</strong>, Sung Ju Hwang, <strong>Juho Lee</strong></li>\n<li>ICML 2021</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2106.06041">arXiv</a>, <a target="_blank" rel="nofollow" href="https://github.com/jmyoon1/adp">code</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://aclanthology.org/2021.acl-long.434/">Learning to perturb word embeddings for out-of-distribution QA</a></p>\n<ul>\n<li>Seanie Lee, Minki Kang, <strong>Juho Lee</strong>, Sung Ju Hwang</li>\n<li>ACL 2021</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2105.02692">arXiv</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://openaccess.thecvf.com/content/CVPR2021/html/Kim_SetVAE_Learning_Hierarchical_Composition_for_Generative_Modeling_of_Set-Structured_Data_CVPR_2021_paper.html">SetVAE: learning hierarchical composition for generative modeling of set-structured data</a></p>\n<ul>\n<li>Jinwoo Kim, Jaehoon Yoo, <strong>Juho Lee</strong>, Seunghoon Hong</li>\n<li>CVPR 2021</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2103.15619">arXiv</a>, <a target="_blank" rel="nofollow" href="https://github.com/jw9730/setvae">code</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://papers.nips.cc/paper/2020/hash/492114f6915a69aa3dd005aa4233ef51-Abstract.html">Bootstrapping neural processes</a></p>\n<ul>\n<li><strong>Juho Lee</strong>*, Yoonho Lee*, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, Yee Whye Teh (*: Equal Contribution)</li>\n<li>NeurIPS 2020</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2008.02956">arXiv</a>, <a target="_blank" rel="nofollow" href="https://github.com/juho-lee/bnp">code</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://papers.nips.cc/paper/2020/hash/6e17a5fd135fcaf4b49f2860c2474c7c-Abstract.html">Neural complexity measures</a></p>\n<ul>\n<li>Yoonho Lee, <strong>Juho Lee</strong>, Sung Ju Hwang, Eunho Yang, Seungjin Choi</li>\n<li>NeurIPS 2020</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2008.02953">arXiv</a>, <a target="_blank" rel="nofollow" href="https://github.com/yoonholee/neural-complexity">code</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://proceedings.mlr.press/v119/heo20a.html">Cost-effective interactive attention learning with neural attention processes</a></p>\n<ul>\n<li>Jay Heo, Junhyeon Park, Hyewon Jeong, Kwang Joon Kim, <strong>Juho Lee</strong>, Eunho Yang, Sung Ju Hwang</li>\n<li>ICML 2020</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2006.05419">arXiv</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://ojs.aaai.org/index.php/AAAI/article/view/5773">Deep mixed effect model using Gaussian processes: a personalized and reliable prediction for healthcare</a></p>\n<ul>\n<li>Ingyo Chung, Saehoon Kim, <strong>Juho Lee</strong>, Sung Ju Hwang, Eunho Yang</li>\n<li>AAAI 2020</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1806.01551">arXiv</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://proceedings.mlr.press/v97/ayed19a.html">Beyond the Chinese restaurant and Pitman-Yor processes: statistical models with double power-law behavior</a></p>\n<ul>\n<li>Fadhel Ayed*, <strong>Juho Lee</strong>*, François Caron (*: Equal Contribution)</li>\n<li>ICML 2019</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1902.04714">arXiv</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://proceedings.mlr.press/v97/lee19d.html">Set transformer: a framework for attention-based permutation-invariant neural networks</a></p>\n<ul>\n<li><strong>Juho Lee</strong>, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, Yee Whye Teh</li>\n<li>ICML 2019</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1810.00825">arXiv</a>, <a target="_blank" rel="nofollow" href="https://github.com/juho-lee/set_transformer">code</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://openreview.net/forum?id=SyVuRiC5K7">Learning to propagate labels: transductive propagation network for few-shot learning</a></p>\n<ul>\n<li>Yanbin Liu, <strong>Juho Lee</strong>, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, Yi Yang</li>\n<li>ICLR 2019</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1805.10002">arXiv</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://proceedings.mlr.press/v89/lee19b.html">A Bayesian model for sparse graphs with flexible degree distribution and overlapping community structure</a></p>\n<ul>\n<li><strong>Juho Lee</strong>, Lancelot F. James, Seungjin Choi, François Caron</li>\n<li>AISTATS 2019</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1810.01778">arXiv</a>, <a target="_blank" rel="nofollow" href="https://github.com/OxCSML-BayesNP/BNRG">code</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://papers.nips.cc/paper/2018/hash/285e19f20beded7d215102b49d5c09a0-Abstract.html">Uncertainty-aware attention for reliable interpretation and prediction</a></p>\n<ul>\n<li>Jay Heo*, Hae Beom Lee*, Saehoon Kim, <strong>Juho Lee</strong>, Kwang Joon Kim, Eunho Yang, Sung Ju Hwang (*: Equal Contribution)</li>\n<li>NeurIPS 2018</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1805.09653">arXiv</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://papers.nips.cc/paper/2018/hash/389bc7bb1e1c2a5e7e147703232a88f6-Abstract.html">Dropmax: adaptive variational softmax</a></p>\n<ul>\n<li>Hae Beom Lee, <strong>Juho Lee</strong>, Saehoon Kim, Eunho Yang, Sung Ju Hwang</li>\n<li>NeurIPS 2018</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1712.07834">arXiv</a>, <a target="_blank" rel="nofollow" href="https://github.com/haebeom-lee/dropmax">code</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="http://proceedings.mlr.press/v70/lee17a.html">Bayesian inference on random simple graphs with power law degree distributions</a></p>\n<ul>\n<li><strong>Juho Lee</strong>, Creighton Heakulani, Zoubin Ghahramani, Lancelot F. James, Seingjin Choi</li>\n<li>ICML 2017</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1702.08239">arXiv</a>, <a target="_blank" rel="nofollow" href="https://github.com/juho-lee/powerlawgraph">code</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://papers.nips.cc/paper/6348-finite-dimensional-bfry-priors-and-variational-bayesian-inference-for-power-law-model">Finite-dimensional BFRY priors and variational Bayesian inference for power law models</a></p>\n<ul>\n<li><strong>Juho Lee</strong>, Lancelot F. James, Seungjin Choi</li>\n<li>NIPS 2016</li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://papers.nips.cc/paper/5800-tree-guided-mcmc-inference-for-normalized-random-measure-mixture-models">Tree-guided MCMC inference for normalized random measure mixture models</a></p>\n<ul>\n<li><strong>Juho Lee</strong>, Seungjin Choi</li>\n<li>NIPS 2015</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1511.05650">arXiv</a>, <a target="_blank" rel="nofollow" href="https://github.com/juho-lee/nrmm.cpp">code</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="http://proceedings.mlr.press/v38/lee15c.html">Bayesian hierarchical clustering with exponential family: small-variance asymptotics and reducibility</a></p>\n<ul>\n<li><strong>Juho Lee</strong>, Seungjin Choi</li>\n<li>AISTATS 2015</li>\n<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1501.07430">arXiv</a></li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="http://proceedings.mlr.press/v33/lee14.html">Incremental tree-based inference with dependent normalized random measures</a></p>\n<ul>\n<li><strong>Juho Lee</strong>, Seungjin Choi</li>\n<li>AISTATS 2014</li>\n</ul>\n</li>\n<li><p><a target="_blank" rel="nofollow" href="https://link.springer.com/chapter/10.1007/978-3-642-33765-9_61">Online video segmentation by Bayesian split-merge clustering</a></p>\n<ul>\n<li><strong>Juho Lee</strong>, Suha Kwak, Bohyung Han, Seungjin Choi</li>\n<li>ECCV 2012</li>\n</ul>\n</li>\n</ul>\n';function m(e){let n,l,m,b=c+"";return{c(){n=a(),l=o("main"),m=o("div"),this.h()},l(e){r('[data-svelte="svelte-1nlpfr9"]',document.head).forEach(i),n=t(e),l=s(e,"MAIN",{});var a=h(l);m=s(a,"DIV",{class:!0}),h(m).forEach(i),a.forEach(i),this.h()},h(){document.title="SIML - Publication",g(m,"class","container")},m(e,a){p(e,n,a),p(e,l,a),u(l,m),m.innerHTML=b},p:f,i:f,o:f,d(e){e&&i(n),e&&i(l)}}}class b extends e{constructor(e){super(),n(this,e,null,m,l,{})}}export{b as default};
